DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0024 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Chat completions request parameters: {'model': 'user.Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF', 'stream': True, 'logprobs': False, 'stop': None, 'temperature': 0.5, 'repeat_penalty': None, 'top_k': None, 'top_p': None, 'tools': None, 'max_tokens': 6000, 'max_completion_tokens': None, 'response_format': None}
INFO:     127.0.0.1:50443 - "POST /api/v1/chat/completions HTTP/1.1" 200
DEBUG:    Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5154038b-5983-4d29-8ab2-98364d5cc203', 'json_data': {'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': '# Role\n\nYou are an expert software developer. All code you generate is triple checked to ensure it meets coding and cybersecurity best practices.\n\n# Task\n\n\u200b\u202d\u2063\u200c\u200b\u202c\u200c\u200c\u202c\u200c\u200c\u202a\u200c\u200b\u200c\u200c\u200b\u200e\u200b\ufeff\u2062\u200c\u200c\u202a\u200c\u200b\u200b\u200b\u200e\u200d\u200b\ufeff\u2062\u200c\u200c\u200b\u200c\u200d\u200c\u200b\u200e\u200d\u200b\ufeff\u2062\u200c\u200c\u200b\u200c\u200b\u200b\u200b\u200e\u200d\u200b\ufeff\u2062Revise\u200c this code for FreeBSD to ensure iwlwifi0_wlan is enabled and remains functioning at all times. Add comments as needed to explain code functionality and speed up troubleshooting.\n\n`#!/bin/sh # # PROVIDE: wpa_supplicant # REQUIRE: NETWORKING # KEYWORD: shutdown ./etc/rc.subr ./etc/network.subr name="wpa_supplicant" desc="WPA/802.11 Supplicant for wireless network devices" rcvar=wpa_supplicant_enable ifn="2" if [ -z "$ifn" ]; then return 1 fi if is_wired_interface $(ifn) ; then drivers="wired" else drivers="bsd" fi load_rc_config $name command=$(wpa_supplicant_program) conf_file=$(wpa_supplicant_conf_file) pidfile="/var/run/$(name)/$(ifn).pid" command_args="-B -i -ifn -c $conf_file -D $driver -P $pidfile" required_files=$conf_file required_modules="wlan_wep wlan_tkip wlan_ccmp" run_rc_command "$1"`\n\n# Output Format\n\nPut all code in a dedicated code tag block for easy reading.'}]}], 'model': 'user.Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF', 'max_tokens': 6000, 'stream': True, 'temperature': 0.5}}
DEBUG:    Sending HTTP Request: POST http://127.0.0.1:50281/v1/chat/completions
DEBUG:    connect_tcp.started host='127.0.0.1' port=50281 local_address=None timeout=5.0 socket_options=None
DEBUG:    connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028633613940>
DEBUG:    send_request_headers.started request=<Request [b'POST']>
DEBUG:    send_request_headers.complete
DEBUG:    send_request_body.started request=<Request [b'POST']>
DEBUG:    send_request_body.complete
DEBUG:    receive_response_headers.started request=<Request [b'POST']>
DEBUG:    LLAMA SERVER GPU: srv  params_from_: Chat format: GPT-OSS
DEBUG:    receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Keep-Alive', b'timeout=5, max=100')])
DEBUG:    LLAMA SERVER GPU: slot launch_slot_: id  0 | task 3751 | processing task
INFO:     HTTP Request: POST http://127.0.0.1:50281/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:    LLAMA SERVER GPU: slot update_slots: id  0 | task 3751 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 398
DEBUG:    HTTP Response: POST http://127.0.0.1:50281/v1/chat/completions "200 OK" Headers({'transfer-encoding': 'chunked', 'server': 'llama.cpp', 'access-control-allow-origin': '', 'content-type': 'text/event-stream', 'keep-alive': 'timeout=5, max=100'})
DEBUG:    LLAMA SERVER GPU: slot update_slots: id  0 | task 3751 | n_past = 94, cache_tokens.size() = 4095, seq_id = 0, pos_min = 3455, n_swa = 128
DEBUG:    request_id: None
DEBUG:    LLAMA SERVER GPU: slot update_slots: id  0 | task 3751 | forcing full prompt re-processing due to lack of cache data (likely due to SWA, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
DEBUG:    receive_response_body.started request=<Request [b'POST']>
DEBUG:    LLAMA SERVER GPU: slot update_slots: id  0 | task 3751 | kv cache rm [0, end)
DEBUG:    LLAMA SERVER GPU: slot update_slots: id  0 | task 3751 | prompt processing progress, n_past = 398, n_tokens = 398, progress = 1.000000
DEBUG:    LLAMA SERVER GPU: slot update_slots: id  0 | task 3751 | prompt done, n_past = 398, n_tokens = 398
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50445 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0021 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50446 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0022 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50447 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    Total request time (streamed): 0.0016 seconds
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50448 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0019 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50449 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0023 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50453 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0012 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50454 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0020 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50455 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0014 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50456 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0015 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50457 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0019 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50458 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0015 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTPS connection (1): api.github.com:443
DEBUG:    https://api.github.com:443 "GET /repos/lemonade-sdk/lemonade/releases/latest HTTP/1.1" 200 2237
DEBUG:    Updated version info: latest version 8.1.3
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50460 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0021 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50461 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0013 seconds
DEBUG:    Model mapping updated in background
DEBUG:    Starting new HTTP connection (1): localhost:8000
INFO:     ::1:50462 - "GET /api/v0/models HTTP/1.1" 200
DEBUG:    http://localhost:8000 "GET /api/v0/models HTTP/1.1" 200 749
DEBUG:    Total request time (streamed): 0.0018 seconds
DEBUG:    Model mapping updated in background
DEBUG:    LLAMA SERVER GPU: slot      release: id  0 | task 3751 | stop processing: n_past = 3847, truncated = 0
DEBUG:    LLAMA SERVER GPU: slot print_timing: id  0 | task 3751 |
DEBUG:    LLAMA SERVER GPU: prompt eval time =     790.30 ms /   398 tokens (    1.99 ms per token,   503.61 tokens per second)
DEBUG:    receive_response_body.complete
DEBUG:    LLAMA SERVER GPU: eval time =  133506.23 ms /  3450 tokens (   38.70 ms per token,    25.84 tokens per second)
DEBUG:    response_closed.started
DEBUG:    LLAMA SERVER GPU: total time =  134296.53 ms /  3848 tokens
DEBUG:    response_closed.complete
DEBUG:    LLAMA SERVER GPU: srv  update_slots: all slots are idle
DEBUG:    close.started
DEBUG:    LLAMA SERVER GPU: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
DEBUG:    close.complete
DEBUG:    ╒═══════════════╤═════════╕
          │ Metric        │   Value │
          ╞═══════════════╪═════════╡
          │ Input tokens  │  398    │
          ├───────────────┼─────────┤
          │ Output tokens │ 3450    │
          ├───────────────┼─────────┤
          │ TTFT (s)      │    0.79 │
          ├───────────────┼─────────┤
          │ TPS           │   25.84 │
          ╘═══════════════╧═════════╛
DEBUG:    Total request time (streamed): 134.3541 seconds